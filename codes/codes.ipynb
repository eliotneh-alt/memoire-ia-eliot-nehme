{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0995b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "636eab23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eliot.nehme\\repositories\\memoire-ia\\codes\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "459cc21c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "could not parse `\"2B\"` as dtype `i64` at column 'DEPT' (column number 6)\n\nThe current offset in the file is 16420 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `schema_overrides` argument\n- setting `ignore_errors` to `True`,\n- adding `\"2B\"` to the `null_values` list.\n\nOriginal error: ```invalid primitive value found during CSV parsing```",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mComputeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# On définit explicitement que DEPT est du texte\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m table_souscription = \u001b[43mpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDonnes_souscription.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(table_souscription)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\polars\\_utils\\deprecation.py:128\u001b[39m, in \u001b[36mdeprecate_renamed_parameter.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(function)\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> T:\n\u001b[32m    125\u001b[39m     _rename_keyword_argument(\n\u001b[32m    126\u001b[39m         old_name, new_name, kwargs, function.\u001b[34m__qualname__\u001b[39m, version\n\u001b[32m    127\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\polars\\io\\csv\\functions.py:551\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(source, has_header, columns, new_columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, use_pyarrow, storage_options, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    544\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m prepare_file_arg(\n\u001b[32m    545\u001b[39m         source,\n\u001b[32m    546\u001b[39m         encoding=encoding,\n\u001b[32m   (...)\u001b[39m\u001b[32m    549\u001b[39m         storage_options=storage_options,\n\u001b[32m    550\u001b[39m     ) \u001b[38;5;28;01mas\u001b[39;00m data:\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m         df = \u001b[43m_read_csv_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    552\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    553\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    554\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    555\u001b[39m \u001b[43m            \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    556\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    557\u001b[39m \u001b[43m            \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    558\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    559\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema_overrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    561\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnull_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnull_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    564\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    565\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    566\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[43m            \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    568\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf8-lossy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m            \u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    577\u001b[39m \u001b[43m            \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    579\u001b[39m \u001b[43m            \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m            \u001b[49m\u001b[43mglob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mglob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    581\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_columns:\n\u001b[32m    584\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _update_columns(df, new_columns)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\polars\\io\\csv\\functions.py:699\u001b[39m, in \u001b[36m_read_csv_impl\u001b[39m\u001b[34m(source, has_header, columns, separator, comment_prefix, quote_char, skip_rows, skip_lines, schema, schema_overrides, null_values, missing_utf8_is_empty_string, ignore_errors, try_parse_dates, n_threads, infer_schema_length, batch_size, n_rows, encoding, low_memory, rechunk, skip_rows_after_header, row_index_name, row_index_offset, sample_size, eol_char, raise_if_empty, truncate_ragged_lines, decimal_comma, glob)\u001b[39m\n\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m    697\u001b[39m projection, columns = parse_columns_arg(columns)\n\u001b[32m--> \u001b[39m\u001b[32m699\u001b[39m pydf = \u001b[43mPyDataFrame\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    700\u001b[39m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    701\u001b[39m \u001b[43m    \u001b[49m\u001b[43minfer_schema_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    703\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    704\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    706\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    707\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprojection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseparator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrechunk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_threads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    714\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    716\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcomment_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquote_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprocessed_null_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmissing_utf8_is_empty_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m    \u001b[49m\u001b[43mskip_rows_after_header\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    724\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparse_row_index_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow_index_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_index_offset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m=\u001b[49m\u001b[43meol_char\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m=\u001b[49m\u001b[43mraise_if_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtruncate_ragged_lines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecimal_comma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(pydf)\n",
      "\u001b[31mComputeError\u001b[39m: could not parse `\"2B\"` as dtype `i64` at column 'DEPT' (column number 6)\n\nThe current offset in the file is 16420 bytes.\n\nYou might want to try:\n- increasing `infer_schema_length` (e.g. `infer_schema_length=10000`),\n- specifying correct dtype with the `schema_overrides` argument\n- setting `ignore_errors` to `True`,\n- adding `\"2B\"` to the `null_values` list.\n\nOriginal error: ```invalid primitive value found during CSV parsing```"
     ]
    }
   ],
   "source": [
    "# On définit explicitement que DEPT est du texte\n",
    "table_souscription = pl.read_csv(\"Donnes_souscription.csv\")\n",
    "\n",
    "print(table_souscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56f4a5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions : (108653, 27)\n",
      "shape: (5, 27)\n",
      "┌─────┬─────────────────┬───────────────────┬─────────────────┬───┬─────┬─────┬─────┬─────┐\n",
      "│     ┆ num_police      ┆ date_debut_police ┆ date_fin_police ┆ … ┆ 2   ┆ 3   ┆ 4   ┆ 5   │\n",
      "│ --- ┆ ---             ┆ ---               ┆ ---             ┆   ┆ --- ┆ --- ┆ --- ┆ --- │\n",
      "│ i64 ┆ i64             ┆ str               ┆ str             ┆   ┆ f64 ┆ f64 ┆ f64 ┆ i64 │\n",
      "╞═════╪═════════════════╪═══════════════════╪═════════════════╪═══╪═════╪═════╪═════╪═════╡\n",
      "│ 2   ┆ 201000000000002 ┆ 2010-08-26        ┆ 2011-08-26      ┆ … ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0   │\n",
      "│ 8   ┆ 201000000000008 ┆ 2010-03-22        ┆ 2011-03-22      ┆ … ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0   │\n",
      "│ 54  ┆ 201000000000054 ┆ 2010-07-06        ┆ 2011-07-06      ┆ … ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0   │\n",
      "│ 56  ┆ 201000000000056 ┆ 2010-05-13        ┆ 2011-05-13      ┆ … ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0   │\n",
      "│ 71  ┆ 201000000000071 ┆ 2010-02-24        ┆ 2011-02-24      ┆ … ┆ 0.0 ┆ 0.0 ┆ 0.0 ┆ 0   │\n",
      "└─────┴─────────────────┴───────────────────┴─────────────────┴───┴─────┴─────┴─────┴─────┘\n"
     ]
    }
   ],
   "source": [
    "# avant de décider du type de chaque colonne. \n",
    "# C'est un peu plus lent (quelques secondes), mais c'est 100% sûr.\n",
    "\n",
    "table_souscription = pl.read_csv(\n",
    "    \"../data/Donnees_souscription.csv\", \n",
    "    infer_schema_length=None  # <--- Le secret est là !\n",
    ")\n",
    "\n",
    "print(f\"Dimensions : {table_souscription.shape}\") # Affiche le nombre de lignes/colonnes\n",
    "print(table_souscription.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a4599ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a292497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0       num_police date_debut_police date_fin_police  REGION DEPT  \\\n",
      "0           2  201000000000002        2010-08-26      2011-08-26      52   72   \n",
      "1           8  201000000000008        2010-03-22      2011-03-22      11   77   \n",
      "2          54  201000000000054        2010-07-06      2011-07-06      52   85   \n",
      "3          56  201000000000056        2010-05-13      2011-05-13      76   66   \n",
      "4          71  201000000000071        2010-02-24      2011-02-24      75   79   \n",
      "\n",
      "  COMMUNE     CRITAIR                       ENERGIE  AGE_VOIT  ... TRANS GARL  \\\n",
      "0   72341  Crit'air 3                       Essence        10  ...     5    1   \n",
      "1   77235  Crit'air 2                        Gazole         5  ...     6    1   \n",
      "2   85226  Crit'air 1  Essence hybride rechargeable         0  ...     5    2   \n",
      "3   66190  Crit'air 3                       Essence        16  ...     Z    2   \n",
      "4   79249  Crit'air 2                        Gazole        16  ...     5    1   \n",
      "\n",
      "   N_COND  ANCIENNETE  N_SINISTRE    1    2    3    4  5  \n",
      "0       3           9           0  0.0  0.0  0.0  0.0  0  \n",
      "1       2           1           0  0.0  0.0  0.0  0.0  0  \n",
      "2       2           0           0  0.0  0.0  0.0  0.0  0  \n",
      "3       3          15           0  0.0  0.0  0.0  0.0  0  \n",
      "4       3           7           0  0.0  0.0  0.0  0.0  0  \n",
      "\n",
      "[5 rows x 27 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/Donnees_souscription.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6c682",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes après explosion (doit correspondre au nombre total de sinistres) : 12349\n",
      "\n",
      "--- Aperçu de la nouvelle table table_sinistres ---\n",
      "         num_police  N_SINISTRE date_debut_police DATE_SINISTRE  COUT_SINISTRE\n",
      "0   201000000000077           1        2010-09-26    2011-06-21        4043.34\n",
      "1   201000000000287           1        2010-03-12    2010-04-17        6235.22\n",
      "2   201000000000321           1        2010-11-05    2011-06-16        1443.37\n",
      "3   201000000000395           1        2010-10-28    2011-10-21         498.43\n",
      "4   201000000000419           1        2010-01-17    2010-06-16        1634.80\n",
      "..              ...         ...               ...           ...            ...\n",
      "95  201000000012791           1        2010-01-06    2010-01-16        3723.03\n",
      "96  201000000012835           1        2010-04-03    2011-03-19        1782.06\n",
      "97  201000000012945           1        2010-07-11    2011-06-26         319.54\n",
      "98  201000000013028           1        2010-11-16    2011-02-09         878.12\n",
      "99  201000000013064           1        2010-03-20    2010-07-29        2483.27\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "\n",
      "--- Aperçu Final avec Images ---\n",
      "        num_police  COUT_SINISTRE  IS_FRAUD  \\\n",
      "0  201000000000077        4043.34         0   \n",
      "1  201000000000287        6235.22         0   \n",
      "2  201000000000321        1443.37         0   \n",
      "3  201000000000395         498.43         0   \n",
      "4  201000000000419        1634.80         0   \n",
      "\n",
      "                         CHEMIN_IMAGE  \n",
      "0  ../data/True_data\\trueimage_59.jpg  \n",
      "1  ../data/True_data\\trueimage_98.jpg  \n",
      "2  ../data/True_data\\trueimage_20.jpg  \n",
      "3  ../data/True_data\\trueimage_50.jpg  \n",
      "4   ../data/True_data\\trueimage_9.jpg  \n",
      "\n",
      "Base sauvegardée sous 'base_sinistres_hybride.csv'. C'est elle que ton IA va lire !\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "import random\n",
    "import os\n",
    "\n",
    "# Chargement de la base de données\n",
    "# J'utilise le chemin vers ton fichier CSV\n",
    "df = pd.read_csv('../data/Donnees_souscription.csv')\n",
    "\n",
    "# --- ÉTAPE 1 : Filtrer et Exploser la base ---\n",
    "\n",
    "# On ne garde que les lignes ayant au moins 1 sinistre\n",
    "# Note : la colonne dans le CSV est \"N_SINISTRE\" (singulier)\n",
    "table_sinistres = df[df['N_SINISTRE'] > 0].copy()\n",
    "\n",
    "# On \"explose\" la table : on duplique les lignes selon la valeur de N_SINISTRE\n",
    "# Si N_SINISTRE = 2, la ligne est répétée 2 fois\n",
    "table_sinistres = table_sinistres.loc[table_sinistres.index.repeat(table_sinistres['N_SINISTRE'])].reset_index(drop=True)\n",
    "\n",
    "print(f\"Nombre de lignes après explosion (doit correspondre au nombre total de sinistres) : {len(table_sinistres)}\")\n",
    "\n",
    "# --- ÉTAPE 2 : Simulation Actuarielle ---\n",
    "\n",
    "# Conversion des colonnes dates en format datetime si ce n'est pas déjà fait\n",
    "table_sinistres['date_debut_police'] = pd.to_datetime(table_sinistres['date_debut_police'])\n",
    "table_sinistres['date_fin_police'] = pd.to_datetime(table_sinistres['date_fin_police'])\n",
    "\n",
    "# 1. Simulation du COUT_SINISTRE (Loi Log-Normale)\n",
    "# Paramètres de la loi normale sous-jacente (mu et sigma)\n",
    "# Ces valeurs (mu=7, sigma=1.5) sont des exemples standards pour obtenir des montants réalistes.\n",
    "# Tu peux les ajuster selon la sévérité moyenne désirée.\n",
    "mu = 7.0\n",
    "sigma = 1.5\n",
    "\n",
    "# Génération des coûts aléatoires\n",
    "table_sinistres['COUT_SINISTRE'] = np.random.lognormal(mean=mu, sigma=sigma, size=len(table_sinistres))\n",
    "\n",
    "# Arrondi à 2 décimales pour faire \"monétaire\"\n",
    "table_sinistres['COUT_SINISTRE'] = table_sinistres['COUT_SINISTRE'].round(2)\n",
    "\n",
    "# 2. Simulation de la DATE_SINISTRE (Aléatoire entre date_debut et date_fin)\n",
    "# Calcul de la durée de couverture en jours pour chaque ligne\n",
    "duree_couverture = (table_sinistres['date_fin_police'] - table_sinistres['date_debut_police']).dt.days\n",
    "\n",
    "# Génération d'un nombre de jours aléatoire entre 0 et la durée de couverture\n",
    "jours_aleatoires = np.random.randint(0, duree_couverture + 1, size=len(table_sinistres))\n",
    "\n",
    "# Ajout de ces jours à la date de début pour obtenir la date du sinistre\n",
    "table_sinistres['DATE_SINISTRE'] = table_sinistres['date_debut_police'] + pd.to_timedelta(jours_aleatoires, unit='D')\n",
    "\n",
    "# --- Affichage et Vérification ---\n",
    "print(\"\\n--- Aperçu de la nouvelle table table_sinistres ---\")\n",
    "print(table_sinistres[['num_police', 'N_SINISTRE', 'date_debut_police', 'DATE_SINISTRE', 'COUT_SINISTRE']].head(100))\n",
    "\n",
    "# Sauvegarde optionnelle\n",
    "# table_sinistres.to_csv('data/table_sinistres_generee.csv', index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- ÉTAPE 3 : Injection de la Fraude et Association des Images ---\n",
    "\n",
    "# 1. Définition des chemins vers tes images (Ceux de ton projet TER)\n",
    "# J'ai repris les chemins de ton PDF, vérifie qu'ils sont toujours bons sur ton PC !\n",
    "path_true_images = \"../data/True_data\"\n",
    "path_fake_images = \"../data/Fake_data\"\n",
    "\n",
    "# Petite sécurité : on vérifie si les dossiers existent, sinon on met un message\n",
    "if not os.path.exists(path_true_images):\n",
    "    print(f\"ATTENTION : Le dossier {path_true_images} est introuvable. Vérifie le chemin.\")\n",
    "    images_reelles_dispo = []\n",
    "else:\n",
    "    # On liste tous les fichiers images (jpg, png) dans le dossier\n",
    "    images_reelles_dispo = [f for f in os.listdir(path_true_images) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "if not os.path.exists(path_fake_images):\n",
    "    print(f\"ATTENTION : Le dossier {path_fake_images} est introuvable.\")\n",
    "    images_fake_dispo = []\n",
    "else:\n",
    "    images_fake_dispo = [f for f in os.listdir(path_fake_images) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "\n",
    "# 2. Simulation de la Fraude\n",
    "# On décide arbitrairement qu'il y a 5% de fraudeurs dans les sinistres\n",
    "# 0 = Honnête, 1 = Fraudeur\n",
    "table_sinistres['IS_FRAUD'] = np.random.choice([0, 1], size=len(table_sinistres), p=[0.95, 0.05])\n",
    "\n",
    "\n",
    "# 3. La fonction magique qui associe l'image\n",
    "def get_image_path(is_fraud):\n",
    "    # Si le dossier est vide ou introuvable, on met \"Image_Manquante\"\n",
    "    if is_fraud == 1:\n",
    "        if not images_fake_dispo: return \"Image_Manquante\"\n",
    "        # On pioche une image au hasard dans le dossier \"Fake\"\n",
    "        nom_image = random.choice(images_fake_dispo)\n",
    "        # On retourne le chemin complet\n",
    "        return os.path.join(path_fake_images, nom_image)\n",
    "    else:\n",
    "        if not images_reelles_dispo: return \"Image_Manquante\"\n",
    "        # On pioche une image au hasard dans le dossier \"True\"\n",
    "        nom_image = random.choice(images_reelles_dispo)\n",
    "        return os.path.join(path_true_images, nom_image)\n",
    "\n",
    "# On applique cette fonction ligne par ligne pour créer la colonne lien\n",
    "# C'est ici que la fusion opère !\n",
    "table_sinistres['CHEMIN_IMAGE'] = table_sinistres['IS_FRAUD'].apply(get_image_path)\n",
    "\n",
    "# --- Résultat Final ---\n",
    "print(\"\\n--- Aperçu Final avec Images ---\")\n",
    "# On regarde les colonnes intéressantes\n",
    "print(table_sinistres[['num_police', 'COUT_SINISTRE', 'IS_FRAUD', 'CHEMIN_IMAGE']].head())\n",
    "\n",
    "# On sauvegarde ce fichier \"Hybride\" pour la suite de ton mémoire\n",
    "table_sinistres.to_csv('base_sinistres_hybride.csv', index=False)\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55554d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting timm\n",
      "  Downloading timm-1.0.24-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: torch in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from timm) (2.10.0)\n",
      "Requirement already satisfied: torchvision in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from timm) (0.25.0)\n",
      "Collecting pyyaml (from timm)\n",
      "  Downloading pyyaml-6.0.3-cp314-cp314-win_amd64.whl.metadata (2.4 kB)\n",
      "Collecting huggingface_hub (from timm)\n",
      "  Downloading huggingface_hub-1.3.4-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting safetensors (from timm)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from huggingface_hub->timm) (3.20.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from huggingface_hub->timm) (2026.1.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface_hub->timm)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from huggingface_hub->timm)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\eliot.nehme\\appdata\\roaming\\python\\python314\\site-packages (from huggingface_hub->timm) (25.0)\n",
      "Collecting shellingham (from huggingface_hub->timm)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting tqdm>=4.42.1 (from huggingface_hub->timm)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typer-slim (from huggingface_hub->timm)\n",
      "  Downloading typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from huggingface_hub->timm) (4.15.0)\n",
      "Collecting anyio (from httpx<1,>=0.23.0->huggingface_hub->timm)\n",
      "  Downloading anyio-4.12.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (2026.1.4)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->huggingface_hub->timm)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from httpx<1,>=0.23.0->huggingface_hub->timm) (3.11)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->huggingface_hub->timm)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\eliot.nehme\\appdata\\roaming\\python\\python314\\site-packages (from tqdm>=4.42.1->huggingface_hub->timm) (0.4.6)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch->timm) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch->timm) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch->timm) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torch->timm) (80.10.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from jinja2->torch->timm) (3.0.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torchvision->timm) (2.4.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\eliot.nehme\\appdata\\local\\python\\pythoncore-3.14-64\\lib\\site-packages (from torchvision->timm) (12.1.0)\n",
      "Collecting click>=8.0.0 (from typer-slim->huggingface_hub->timm)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Downloading timm-1.0.24-py3-none-any.whl (2.6 MB)\n",
      "   ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/2.6 MB 12.8 MB/s  0:00:00\n",
      "Downloading huggingface_hub-1.3.4-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.6/536.6 kB 11.7 MB/s  0:00:00\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 15.7 MB/s  0:00:00\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading pyyaml-6.0.3-cp314-cp314-win_amd64.whl (156 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading anyio-4.12.1-py3-none-any.whl (113 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Installing collected packages: tqdm, shellingham, safetensors, pyyaml, hf-xet, h11, click, anyio, typer-slim, httpcore, httpx, huggingface_hub, timm\n",
      "\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   ----------------------------------------  0/13 [tqdm]\n",
      "   --- ------------------------------------  1/13 [shellingham]\n",
      "   ------ ---------------------------------  2/13 [safetensors]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   --------- ------------------------------  3/13 [pyyaml]\n",
      "   --------------- ------------------------  5/13 [h11]\n",
      "   --------------- ------------------------  5/13 [h11]\n",
      "   --------------- ------------------------  5/13 [h11]\n",
      "   ------------------ ---------------------  6/13 [click]\n",
      "   ------------------ ---------------------  6/13 [click]\n",
      "   ------------------ ---------------------  6/13 [click]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   --------------------- ------------------  7/13 [anyio]\n",
      "   ------------------------ ---------------  8/13 [typer-slim]\n",
      "   ------------------------ ---------------  8/13 [typer-slim]\n",
      "   ------------------------ ---------------  8/13 [typer-slim]\n",
      "   --------------------------- ------------  9/13 [httpcore]\n",
      "   --------------------------- ------------  9/13 [httpcore]\n",
      "   --------------------------- ------------  9/13 [httpcore]\n",
      "   --------------------------- ------------  9/13 [httpcore]\n",
      "   --------------------------- ------------  9/13 [httpcore]\n",
      "   --------------------------- ------------  9/13 [httpcore]\n",
      "   ------------------------------ --------- 10/13 [httpx]\n",
      "   ------------------------------ --------- 10/13 [httpx]\n",
      "   ------------------------------ --------- 10/13 [httpx]\n",
      "   ------------------------------ --------- 10/13 [httpx]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   --------------------------------- ------ 11/13 [huggingface_hub]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ------------------------------------ --- 12/13 [timm]\n",
      "   ---------------------------------------- 13/13 [timm]\n",
      "\n",
      "Successfully installed anyio-4.12.1 click-8.3.1 h11-0.16.0 hf-xet-1.2.0 httpcore-1.0.9 httpx-0.28.1 huggingface_hub-1.3.4 pyyaml-6.0.3 safetensors-0.7.0 shellingham-1.5.4 timm-1.0.24 tqdm-4.67.1 typer-slim-0.21.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'c:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts hf.exe and tiny-agents.exe are installed in 'c:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "%pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32ebe7b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utilisation du device : cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement du modèle ViT...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eliot.nehme\\AppData\\Local\\Python\\pythoncore-3.14-64\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\eliot.nehme\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Début du scoring des sinistres (ça peut être long)...\n",
      "Scoring terminé !\n",
      "         num_police  IS_FRAUD  SCORE_IA\n",
      "0   201000000000077         0  0.533123\n",
      "1   201000000000287         0  0.309675\n",
      "2   201000000000321         0  0.858065\n",
      "3   201000000000395         0  0.942666\n",
      "4   201000000000419         0  0.835617\n",
      "5   201000000000620         0  0.634176\n",
      "6   201000000000636         0  0.903919\n",
      "7   201000000000650         1  0.071488\n",
      "8   201000000000685         0  0.343526\n",
      "9   201000000000738         0  0.877745\n",
      "10  201000000000764         0  0.490846\n",
      "11  201000000000808         0  0.444288\n",
      "12  201000000000888         0  0.661286\n",
      "13  201000000000918         0  0.679870\n",
      "14  201000000001026         0  0.178379\n",
      "15  201000000001304         0  0.638237\n",
      "16  201000000001356         0  0.768726\n",
      "17  201000000001640         0  0.343010\n",
      "18  201000000001692         0  0.513530\n",
      "19  201000000001983         0  0.497779\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import os\n",
    "\n",
    "# --- 1. CONFIGURATION ---\n",
    "# Charge ta base hybride créée juste avant\n",
    "df = pd.read_csv('base_sinistres_hybride.csv')\n",
    "\n",
    "# Définis si tu utilises le GPU ou le CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Utilisation du device : {device}\")\n",
    "\n",
    "# --- 2. CHARGEMENT DE TON MODÈLE IA (C'est ici que tu utilises tes codes) ---\n",
    "\n",
    "# A. Définis la transformation de l'image (copie celle de ton code d'entraînement)\n",
    "# Généralement, c'est quelque chose comme ça :\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), # Taille standard pour ViT\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# B. Charge ton modèle\n",
    "# SI TU AS LE FICHIER .PT (le modèle entraîné) :\n",
    "# model = torch.load('ton_modele_entraine.pt') \n",
    "\n",
    "# SI TU N'AS PAS LE MODÈLE SOUS LA MAIN POUR L'INSTANT :\n",
    "# On va utiliser un modèle pré-entraîné juste pour que le code tourne pour le mémoire\n",
    "# (C'est une astuce pour avancer : on utilise un modèle générique)\n",
    "import timm\n",
    "print(\"Chargement du modèle ViT...\")\n",
    "model = timm.create_model('vit_base_patch16_224', pretrained=True, num_classes=2) \n",
    "# Note : num_classes=2 car (0=Sain, 1=Fraude)\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval() # Important : met le modèle en mode \"Evaluation\" (pas d'apprentissage)\n",
    "\n",
    "# --- 3. LA FONCTION DE DÉTECTION ---\n",
    "\n",
    "def detecter_fraude(chemin_image):\n",
    "    \"\"\"\n",
    "    Prend un chemin d'image, et retourne la probabilité de fraude (entre 0 et 1).\n",
    "    \"\"\"\n",
    "    # Si pas d'image, on ne peut rien dire (score neutre ou -1)\n",
    "    if pd.isna(chemin_image) or chemin_image == \"Image_Manquante\":\n",
    "        return 0.0\n",
    "    \n",
    "    try:\n",
    "        # 1. Ouvrir l'image\n",
    "        img = Image.open(chemin_image).convert('RGB')\n",
    "        \n",
    "        # 2. Transformer l'image pour l'IA\n",
    "        img_t = transform(img).unsqueeze(0).to(device) # Ajoute une dimension batch\n",
    "        \n",
    "        # 3. Prédiction\n",
    "        with torch.no_grad(): # On ne calcule pas les gradients (plus rapide)\n",
    "            output = model(img_t)\n",
    "            # On applique Softmax pour avoir une probabilité\n",
    "            probabilities = torch.nn.functional.softmax(output, dim=1)\n",
    "            \n",
    "            # On suppose que la classe 1 est \"Fraude\" (vérifie tes codes TER !)\n",
    "            score_fraude = probabilities[0][1].item()\n",
    "            \n",
    "        return score_fraude\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur sur l'image {chemin_image} : {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# --- 4. EXÉCUTION SUR TOUTE LA BASE ---\n",
    "print(\"Début du scoring des sinistres (ça peut être long)...\")\n",
    "\n",
    "# On applique la fonction sur chaque ligne\n",
    "# Pour tester vite fait, tu peux ajouter .head(50) à la fin de df['CHEMIN_IMAGE']\n",
    "df['SCORE_IA'] = df['CHEMIN_IMAGE'].apply(detecter_fraude)\n",
    "\n",
    "# --- 5. SAUVEGARDE ET ANALYSE ---\n",
    "print(\"Scoring terminé !\")\n",
    "\n",
    "# On regarde les résultats\n",
    "print(df[['num_police', 'IS_FRAUD', 'SCORE_IA']].head(20))\n",
    "\n",
    "# Sauvegarde finale\n",
    "df.to_csv('base_scoree_finale.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b17c00d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, classification_report, roc_auc_score\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 1. Charger ton fichier de résultats\n",
    "df = pd.read_csv('base_scoree_finale.csv')\n",
    "\n",
    "# 2. Définir le seuil de décision\n",
    "# L'IA donne une proba (ex: 0.89). Si > 0.5, on dit \"C'est une fraude\".\n",
    "seuil = 0.5\n",
    "df['PREDICTION_CLASSE'] = (df['SCORE_IA'] > seuil).astype(int)\n",
    "\n",
    "# 3. Calculer la Matrice de Confusion\n",
    "# C'est LE tableau le plus important de ton mémoire\n",
    "conf_matrix = confusion_matrix(df['IS_FRAUD'], df['PREDICTION_CLASSE'])\n",
    "\n",
    "# Affichage propre\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Prédit Non-Fraude', 'Prédit Fraude'],\n",
    "            yticklabels=['Réel Non-Fraude', 'Réel Fraude'])\n",
    "plt.title('Matrice de Confusion (Performance de l\\'IA)')\n",
    "plt.ylabel('Vérité Terrain')\n",
    "plt.xlabel('Prédiction de l\\'IA')\n",
    "plt.show()\n",
    "\n",
    "# 4. Les Métriques Actuarielles\n",
    "print(\"\\n--- RAPPORT DE PERFORMANCE ---\")\n",
    "print(classification_report(df['IS_FRAUD'], df['PREDICTION_CLASSE']))\n",
    "\n",
    "auc = roc_auc_score(df['IS_FRAUD'], df['SCORE_IA'])\n",
    "print(f\"AUC Score (Capacité de tri global) : {auc:.4f}\")\n",
    "# Si AUC = 0.5 -> Ton IA joue à pile ou face (Nul).\n",
    "# Si AUC > 0.8 -> Ton IA est excellente.\n",
    "\n",
    "# 5. Calcul Financier (Pour le Boss)\n",
    "# Imaginons : Un sinistre moyen coûte 1500€. Une enquête inutile coûte 200€.\n",
    "cout_moyen_sinistre = 1500\n",
    "cout_enquete = 200\n",
    "\n",
    "# Vrais Positifs (Fraudes arrêtées) : On économise le sinistre - le coût d'enquête\n",
    "VP = conf_matrix[1, 1]\n",
    "economie = VP * (cout_moyen_sinistre - cout_enquete)\n",
    "\n",
    "# Faux Positifs (Clients honnêtes embêtés) : On perd juste le coût d'enquête\n",
    "FP = conf_matrix[0, 1]\n",
    "perte_inutile = FP * cout_enquete\n",
    "\n",
    "gain_net = economie - perte_inutile\n",
    "\n",
    "print(f\"\\n--- IMPACT FINANCIER SIMULÉ ---\")\n",
    "print(f\"Fraudes correctement bloquées : {VP}\")\n",
    "print(f\"Clients honnêtes suspectés à tort : {FP}\")\n",
    "print(f\"Gain net estimé pour l'assureur : {gain_net} €\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
